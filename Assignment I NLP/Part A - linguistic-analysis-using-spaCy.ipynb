{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c451f7ee",
   "metadata": {},
   "source": [
    "# PART A:  Linguistic analysis using spaCy \n",
    "\n",
    "In the first part of the assignment, we focus on an analysis of the sentences in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f462ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e169d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file = open(\"../datas/sentences.txt\", encoding='utf-8')\n",
    "data = df_file.read()\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99640ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data) # nlp object is used to create documents with linguistic annotations\n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b29e8",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94d47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    \n",
    "    token_list = []\n",
    "    \n",
    "    for token in text:\n",
    "        token_list.append(token.text)\n",
    "        \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a71647c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n['children',\\n'are',\\n'thought',\\n'to',\\n'be',\\n'aged',\\n'three',...]\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # tokenization(doc)\n",
    "\"\"\"\n",
    "['children',\n",
    " 'are',\n",
    " 'thought',\n",
    " 'to',\n",
    " 'be',\n",
    " 'aged',\n",
    " 'three',...]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be70f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_tokens(text):\n",
    "    \n",
    "    total_token = 0\n",
    "    for token in text:\n",
    "        total_token += 1\n",
    "    \n",
    "    return total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf93ef6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16130"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_tokens(doc) # 16130 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab01bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_types(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Type is different from the number of actual occurrences which would be known as tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    token_list = set(tokenization(text))\n",
    "    return len(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3a5325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3746"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_types(doc) # 3746 types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0357cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_words(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    A word is a speech sound or a combination of sounds, or its representation in writing, \n",
    "    that symbolizes and communicates a meaning and may consist of a single morpheme or a combination of morphemes.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_words = 0\n",
    "    words = []\n",
    "    \n",
    "    for token in text:\n",
    "        if token.text not in punctuations and token.text != '\\n':\n",
    "            total_words += 1\n",
    "            words.append(token.text)\n",
    "    \n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2181b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_words(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    A word is a speech sound or a combination of sounds, or its representation in writing, \n",
    "    that symbolizes and communicates a meaning and may consist of a single morpheme or a combination of morphemes.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_words = 0\n",
    "    words = []\n",
    "    \n",
    "    for token in text:\n",
    "        if token.text not in punctuations and token.text != '\\n':\n",
    "            total_words += 1\n",
    "            words.append(token.text)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039a282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_words(doc)# 13265 words\n",
    "# list_of_words(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e4da8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_num_of_words_per_sentence(text):\n",
    "    \n",
    "    sentence_count = 0\n",
    "    \n",
    "    for sent in text.sents:\n",
    "        sentence_count += 1\n",
    "\n",
    "\n",
    "    avg_num_words = (num_of_words(text))  / sentence_count\n",
    "    print(num_of_words(text),sentence_count)\n",
    "    \n",
    "    return round(avg_num_words,2) # 23.63 number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb4ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13265 718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.47"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_num_of_words_per_sentence(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb5f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "\n",
    "    words_list = list_of_words(text)\n",
    "    avg_num_words = 0\n",
    "    \n",
    "    avg_num_words = sum(len(word) for word in words_list) / len(words_list)            \n",
    "    \n",
    "    return round(avg_num_words,2) # 4.9 average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c7b8ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_word_length(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f05f4",
   "metadata": {},
   "source": [
    "### 2. Word Classes \n",
    "\n",
    "Run the default part-of-speech tagger on the dataset and identify the ten most frequent `POS tags`. Complete the table for these ten tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1aa0b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\tPOS\tTAG\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"TOKEN\\tPOS\\tTAG\".format('Token','Lemma','Pos'))\n",
    "print(\"-\"*50)\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.text, token.pos_, token.tag_)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "TOKEN    POS    TAG\n",
    "--------------------------------------------------\n",
    "children NOUN NNS\n",
    "are AUX VBP\n",
    "thought VERB VBN\n",
    "to PART TO\n",
    "be AUX VB\n",
    "aged VERB VBN\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d4d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 2074, 'NNP': 2063, 'IN': 1745, 'DT': 1378, 'JJ': 868, 'NNS': 774, ',': 699, 'VBD': 660, '.': 655, '_SP': 653, 'VBN': 500, 'RB': 451, 'CD': 357, 'CC': 347, 'PRP': 338, 'VB': 328, 'VBZ': 301, 'VBG': 296, \"''\": 258, 'VBP': 202, 'TO': 182, 'PRP$': 145, 'POS': 111, 'HYPH': 105, 'MD': 93, 'WDT': 74, ':': 63, '-LRB-': 57, '-RRB-': 57, 'WRB': 46, 'RP': 40, 'WP': 38, 'NNPS': 35, 'JJS': 27, 'JJR': 23, 'RBS': 18, 'RBR': 18, 'EX': 14, 'NFP': 8, 'UH': 7, 'XX': 6, '$': 5, 'SYM': 4, 'PDT': 3, 'FW': 2, '``': 2})\n"
     ]
    }
   ],
   "source": [
    "tag_frequencies = Counter()\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    tags = []\n",
    "    for token in sentence: \n",
    "        #if not token.is_punct:\n",
    "        tags.append(token.tag_)\n",
    "    tag_frequencies.update(tags)\n",
    "    \n",
    "print(tag_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa466dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequencies = Counter()\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    tokens = []\n",
    "    token_list = ['NN','NNP','IN','DT','JJ','NNS', ',','VBD','.','_SP','VBN','RB','CD']\n",
    "    for token in sentence: \n",
    "        if token.tag_ in token_list:\n",
    "            tokens.append((token.tag_,token.text))\n",
    "    token_frequencies.update(tokens)\n",
    "    \n",
    "#print(token_frequencies)\n",
    "\n",
    "\n",
    "\n",
    "# Counter({('DT', 'the'): 723, (',', ','): 697, ('_SP', '\\n'): 653, ('.', '.'): 638, ('IN', 'of'): 352, ('IN', 'in'): 281,\n",
    "#('DT', 'a'): 279, ('NNP', '\\\\'): 211, ('IN', 'to'): 153, ('DT', 'The'): 124, ('VBD', 'was'): 111, ('IN', 'on'): 110, \n",
    "# ('IN', 'for'): 102, ('IN', 'with'): 85, ('IN', 'at'): 75, ('IN', 'as'): 69, ('IN', 'from'): 65, ('VBD', 'were'): 62})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1094023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IN', 0.021739130434782608), ('JJ', 0.021739130434782608), ('DT', 0.021739130434782608), ('NNP', 0.0), ('NNS', 0.0), ('VBN', 0.0), ('NN', 0.021739130434782608), ('CD', 0.0), ('VBD', 0.0), ('_SP', 0.043478260869565216), ('RB', 0.0), (',', 0.0), ('.', 0.021739130434782608)]\n"
     ]
    }
   ],
   "source": [
    "taglist_frequencies = Counter()\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    tag_list = []\n",
    "    for token in sentence:\n",
    "        tag_list.append(token.tag_)\n",
    "    taglist_frequencies.update(tag_list)\n",
    "\n",
    "#print(tag_list)\n",
    "#print(taglist_frequencies)\n",
    "\n",
    "\n",
    "counts = [(token.tag_, tag_list.count(token.tag_) / len(taglist_frequencies)) for token.tag_ in set(token_list)] \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d71ba0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : find the 3 most frequent ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35b5f246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, singular or mass'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641173fe",
   "metadata": {},
   "source": [
    "### 3. N-Grams\n",
    "\n",
    "N-grams are a combination of N tokens that usually co-occur. For example, the word new occurs in a lot of contexts, but the word york frequently occurs with new. So we combine the two and get `new york` to give better information. Combining 2 tokens(unigrams) gives us a **bigram**. Higher order **n-grams** are formed using 2 (n-1)-grams. 2 bigrams give a **trigram**, 2 trigrams form a quadgram and so on.\n",
    "\n",
    "Calculate the distribution of n-grams and provide the 3 most frequent.\n",
    " - `Token bigrams`\n",
    " - `Token trigrams`\n",
    " - `POS bigrams`\n",
    " - `POS trigrams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f2e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for chunk in doc.noun_chunks:\n",
    "#    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "184c8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text,tokens,n):\n",
    "    tokens = [token.text for token in text]\n",
    "    \n",
    "    return [tokens[i:i+n] for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d707a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_bigrams = n_grams(doc,tokens,2)\n",
    "token_trigrams = n_grams(doc,tokens,4)\n",
    "#print(token_bigrams) # ['were', 'hospitalised'], ['military', 'presence']\n",
    "#print(token_trigrams) # ['children', 'are', 'thought', 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7351a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    item = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency > counter):\n",
    "            counter = curr_frequency\n",
    "            item = i\n",
    " \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9945ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent(token_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2174f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '\\\\', '\"', '\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent(token_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f643984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : find the POS bigrams and POS trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67aca34",
   "metadata": {},
   "source": [
    "### 4. Lemmatization  \n",
    "\n",
    "Provide an example for a lemma that occurs in `more than two inflections` in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77f68819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    \n",
    "    for token in text:\n",
    "        if token.pos_ == 'VERB':\n",
    "            print('{} -> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d970ccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthought -> think\\naged -> age\\nmixed -> mix\\nplated -> plate\\ngrow -> grow\\nfeel -> feel\\nrepresented -> represent\\nsuffering -> suffer\\nconcerns -> concern\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization(doc)\n",
    "\n",
    "\"\"\"\n",
    "thought -> think\n",
    "aged -> age\n",
    "mixed -> mix\n",
    "plated -> plate\n",
    "grow -> grow\n",
    "feel -> feel\n",
    "represented -> represent\n",
    "suffering -> suffer\n",
    "concerns -> concern\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c487478",
   "metadata": {},
   "source": [
    "# 5. Named Entity Recognition\n",
    "\n",
    "Analyze the `named entities` in the *first five sentences*. Are they identified correctly? If not, explain your answer and propose a better decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a8dc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(text):\n",
    "    list_of_ent = []\n",
    "    \n",
    "    for ent in text.ents:\n",
    "        list_of_ent.append(ent.label_)\n",
    "        print(ent.text, ent.label_)\n",
    "    print(len(list_of_ent)) # 1627 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b1e25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner(doc)\n",
    "\n",
    "\n",
    "# three , eight DATE\n",
    "# ten years DATE\n",
    "# eighteen-month-old DATE\n",
    "# ROS GPE\n",
    "# third ORDINAL\n",
    "# three CARDINAL\n",
    "# \\ ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e188f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_different(text):\n",
    "    list_of_diff_ent = []\n",
    "    \n",
    "    for ent in text.ents:\n",
    "        list_of_diff_ent.append(ent.label_)\n",
    "    print(len(set(list_of_diff_ent))) \n",
    "    \n",
    "# {'LAW', 'GPE', 'QUANTITY', 'MONEY', 'NORP', 'WORK_OF_ART', 'ORDINAL', 'TIME', 'CARDINAL', 'DATE', \n",
    "# 'ORG', 'PRODUCT', 'PERSON', 'EVENT', 'FAC', 'PERCENT', 'LOC'} \n",
    "\n",
    "# 17 different entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08e7e0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "ner_different(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf819011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ORG') # not correct for \"\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dffdbb",
   "metadata": {},
   "source": [
    "### Analyze the named entities in the first five sentences. Are they identified correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "558118da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_five_sentence(text):\n",
    "    \n",
    "    first_five_sentence = []\n",
    "    for sentence in doc.sents:\n",
    "        first_five_sentence.append(sentence)\n",
    "\n",
    "    \n",
    "    return(first_five_sentence[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1abc867a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[children are thought to be aged three , eight , and ten years , alongside an eighteen-month-old baby ., \\nWe mixed different concentrations of ROS with the spores , plated them out on petridishes with an agar-solution where fungus can grow on ., \\nThey feel they are under-represented in higher education and are suffering in a regional economic downturn ., \\nEspecially as it concerns a third party building up its military presence near our borders ., \\nPolice said three children were hospitalised for \\\\\" severe dehydration \\\\\" .]'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_sentence_list = str(first_five_sentence(doc))\n",
    "first_five_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40659f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_five_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8865d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('three , eight', 'DATE'), ('ten years', 'DATE'), ('eighteen-month-old', 'DATE'), ('ROS', 'GPE'), ('third', 'ORDINAL'), ('three', 'CARDINAL'), ('\\\\', 'ORG')]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "ner_first_five = []\n",
    "doc_5 = nlp(first_five_sentence_list)\n",
    "for ent in doc_5.ents:\n",
    "    ner_first_five.append((ent.text,ent.label_))\n",
    "        \n",
    "print(ner_first_five)\n",
    "print(len(ner_first_five))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
