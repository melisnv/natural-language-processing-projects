{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # This loads a small English model trained on web data.\n",
    "# creating the spaCy object 'nlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this object to process text through a defined pipeline of modules and store the result as a value for another variable for accessing it. The results is another spaCy object of the type 'Doc' which gives us access to all the different analyses of the pipeline through different functions. In a Doc object we can access tokens, their lemmas, their PoS, sentences, chunks, named entities, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"In a video on social media, he said there was now a culture where a claim is made with the idea that a settlement will be cheaper than taking it to court, even if there's no basis for the claim\"\n",
    "# running nlp pipeline on the test_input\n",
    "doc = nlp(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"It was an extraordinarily good day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = nlp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cb17456253cd426588504b88ed011775-0\" class=\"displacy\" width=\"1100\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">extraordinarily</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">day.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cb17456253cd426588504b88ed011775-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cb17456253cd426588504b88ed011775-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cb17456253cd426588504b88ed011775-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cb17456253cd426588504b88ed011775-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cb17456253cd426588504b88ed011775-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cb17456253cd426588504b88ed011775-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cb17456253cd426588504b88ed011775-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cb17456253cd426588504b88ed011775-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cb17456253cd426588504b88ed011775-0-4\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cb17456253cd426588504b88ed011775-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(preprocessed, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization\n",
    "\n",
    "The basic unit in NLP is usually the token. Punctuation is treated as a separate token and check how \"It's\" is tokenized. Try a few other test inputs to better understand the concept of a token. Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. Punctuation marks, words, and numbers can be considered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 In 0\n",
      "1 a 3\n",
      "2 video 5\n",
      "3 on 11\n",
      "4 social 14\n",
      "5 media 21\n",
      "6 , 26\n",
      "7 he 28\n",
      "8 said 31\n",
      "9 there 36\n",
      "10 was 42\n",
      "11 now 46\n",
      "12 a 50\n",
      "13 culture 52\n",
      "14 where 60\n",
      "15 a 66\n",
      "16 claim 68\n",
      "17 is 74\n",
      "18 made 77\n",
      "19 with 82\n",
      "20 the 87\n",
      "21 idea 91\n",
      "22 that 96\n",
      "23 a 101\n",
      "24 settlement 103\n",
      "25 will 114\n",
      "26 be 119\n",
      "27 cheaper 122\n",
      "28 than 130\n",
      "29 taking 135\n",
      "30 it 142\n",
      "31 to 145\n",
      "32 court 148\n",
      "33 , 153\n",
      "34 even 155\n",
      "35 if 160\n",
      "36 there 163\n",
      "37 's 168\n",
      "38 no 171\n",
      "39 basis 174\n",
      "40 for 180\n",
      "41 the 184\n",
      "42 claim 188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.i, token, token.idx)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 It 0\n",
      "1 was 3\n",
      "2 an 7\n",
      "3 extraordinarily 10\n",
      "4 good 26\n",
      "5 day 31\n",
      "6 . 34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in preprocessed:\n",
    "    print(token.i, token, token.idx)\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>spaCy</b> provides sentence segmentation by grouping tokens together. Try different test inputs to analyze the quality of the sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a video on social media, he said there was now a culture where a claim is made with the idea that a settlement will be cheaper than taking it to court, even if there's no basis for the claim\n",
      "In\n",
      "a\n",
      "video\n",
      "on\n",
      "social\n",
      "media\n",
      ",\n",
      "he\n",
      "said\n",
      "there\n",
      "was\n",
      "now\n",
      "a\n",
      "culture\n",
      "where\n",
      "a\n",
      "claim\n",
      "is\n",
      "made\n",
      "with\n",
      "the\n",
      "idea\n",
      "that\n",
      "a\n",
      "settlement\n",
      "will\n",
      "be\n",
      "cheaper\n",
      "than\n",
      "taking\n",
      "it\n",
      "to\n",
      "court\n",
      ",\n",
      "even\n",
      "if\n",
      "there\n",
      "'s\n",
      "no\n",
      "basis\n",
      "for\n",
      "the\n",
      "claim\n"
     ]
    }
   ],
   "source": [
    "sentences = doc.sents\n",
    "\n",
    "for sentence in sentences:\n",
    "    print()\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It was an extraordinarily good day.\n",
      "It\n",
      "was\n",
      "an\n",
      "extraordinarily\n",
      "good\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentence_two = preprocessed.sents\n",
    "\n",
    "for sentence in sentence_two:\n",
    "    print()\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lemmatization\n",
    "\n",
    "Lemmatization is the process of finding the form of the related word in the dictionary. It is different from Stemming. It involves longer processes to calculate than Stemming.\n",
    "\n",
    "When we apply the <b>‘lemmatize’</b> process to the word <b>‘made’</b>, it change it and reaches the word <b>‘make’</b>, which is the dictionary form of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In in\n",
      "a a\n",
      "video video\n",
      "on on\n",
      "social social\n",
      "media medium\n",
      ", ,\n",
      "he he\n",
      "said say\n",
      "there there\n",
      "was be\n",
      "now now\n",
      "a a\n",
      "culture culture\n",
      "where where\n",
      "a a\n",
      "claim claim\n",
      "is be\n",
      "made make\n",
      "with with\n",
      "the the\n",
      "idea idea\n",
      "that that\n",
      "a a\n",
      "settlement settlement\n",
      "will will\n",
      "be be\n",
      "cheaper cheap\n",
      "than than\n",
      "taking take\n",
      "it it\n",
      "to to\n",
      "court court\n",
      ", ,\n",
      "even even\n",
      "if if\n",
      "there there\n",
      "'s be\n",
      "no no\n",
      "basis basis\n",
      "for for\n",
      "the the\n",
      "claim claim\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Part of Speech Tagging (POS-Tagging)\n",
    "\n",
    "POS-Tag is the labeling of the words in a text according to their word types (noun, adjective, adverb, verb, etc.). A part-of-speech tagger assigns a word class to each token. The number of word classes depends on the tagset that the model uses. POS tagging is a supervised learning solution that uses features like the previous word, next word, is first letter capitalized etc. When applying this, we first need to split a sentence into tokens. Tagging works after splitting to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ADP IN\n",
      "a DET DT\n",
      "video NOUN NN\n",
      "on ADP IN\n",
      "social ADJ JJ\n",
      "media NOUN NNS\n",
      ", PUNCT ,\n",
      "he PRON PRP\n",
      "said VERB VBD\n",
      "there PRON EX\n",
      "was VERB VBD\n",
      "now ADV RB\n",
      "a DET DT\n",
      "culture NOUN NN\n",
      "where SCONJ WRB\n",
      "a DET DT\n",
      "claim NOUN NN\n",
      "is AUX VBZ\n",
      "made VERB VBN\n",
      "with ADP IN\n",
      "the DET DT\n",
      "idea NOUN NN\n",
      "that SCONJ IN\n",
      "a DET DT\n",
      "settlement NOUN NN\n",
      "will AUX MD\n",
      "be AUX VB\n",
      "cheaper ADJ JJR\n",
      "than ADP IN\n",
      "taking VERB VBG\n",
      "it PRON PRP\n",
      "to ADP IN\n",
      "court NOUN NN\n",
      ", PUNCT ,\n",
      "even ADV RB\n",
      "if SCONJ IN\n",
      "there PRON EX\n",
      "'s VERB VBZ\n",
      "no DET DT\n",
      "basis NOUN NN\n",
      "for ADP IN\n",
      "the DET DT\n",
      "claim NOUN NN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, 3rd person singular present'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"VBZ\") # SpaCy provides a short explanation for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token = doc[0]\n",
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes without <b>_</b> return numerical values which spaCy uses internally. Variants with <b>_</b> provide the human readable rendering of the value in unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292078113972184607 IN\n"
     ]
    }
   ],
   "source": [
    "print(first_token.tag, first_token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Named Entity Recognition\n",
    "\n",
    "Named entity recognition is a natural language processing technique that can automatically scan entire articles and pull out some fundamental entities in a text and classify them into predefined categories. Named Entity Recognition is the process of detecting the named entities such as person names, location names, company names, etc from the text. It is also known as entity identification or entity extraction or entity chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"But Google is starting from behind. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\"\"\"\n",
    "doc2 = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Apple’s Siri ORG\n",
      "iPhones ORG\n",
      "Amazon’s Alexa ORG\n",
      "Echo GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple’s Siri\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", available on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon’s Alexa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Echo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and Dot devices, have clear leads in consumer adoption.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc2, jupyter=True, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calculating Frequencies\n",
    "\n",
    "A common analysis step for language corpora is the extraction of frequency statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'’s': 83, 'to': 69, 'raw': 62, 'text': 58, 'it': 56, 'is': 51, 'words': 44, 'in': 33, 'and': 32, 'That': 31, 'Processing': 29, 'difficult': 29, 'exactly': 29, 'While': 28, 'what': 28, 'spaCy': 27, 'intelligently': 26, 'a': 26, 'possible': 25, 'designed': 25, 'solve': 23, 'do': 23, 'most': 22, 'some': 22, 'the': 21, 'problems': 21, 'you': 21, 'are': 20, 'starting': 20, 'put': 20, 'rare': 19, 'that': 19, 'from': 19, 'different': 18, 'only': 18, 'Even': 16, 'same': 15, 'splitting': 15, 'useful': 15, 'characters': 15, 'common': 14, 'get': 14, 'for': 13, 'The': 13, 'can': 13, 'into': 13, 'back': 13, 'completely': 12, 'mean': 11, 'word': 11, 'usually': 11, 'Doc': 11, 'look': 10, 'better': 10, 'object': 10, 'like': 9, 'units': 8, 'use': 8, 'order': 7, 'linguistic': 7, 'comes': 7, 'be': 6, 'knowledge': 6, 'with': 6, 'almost': 5, 'something': 4, 'add': 4, 'variety': 4, 'many': 3, 'of': 3, 'thing': 2, 'languages': 2, 'information': 2, 'annotations': 2})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "test_input2 = \"Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it’s possible to solve some problems starting from only the raw characters, it’s usually better to use linguistic knowledge to add useful information. That’s exactly what spaCy is designed to do: you put in raw text, and get back a Doc object, that comes with a variety of annotations.\"\n",
    "doc3 = nlp(test_input2) # running the NLP pipeline on the test input\n",
    "\n",
    "word_frequencies = Counter()\n",
    "\n",
    "for sentence in doc3.sents:\n",
    "    words = []\n",
    "    \n",
    "    for token in sentence:\n",
    "        # filtering out the punctuation\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "        word_frequencies.update(words)\n",
    "    \n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 1447 74\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(doc)\n",
    "num_words = sum(word_frequencies.values())\n",
    "num_types = len(word_frequencies.keys())\n",
    "\n",
    "print(num_tokens, num_words, num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
