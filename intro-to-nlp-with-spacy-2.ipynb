{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5c01fa",
   "metadata": {},
   "source": [
    "# Intro to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8cf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fb7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The social media firm's communications team tweeted: 'Now that everyone is asking… yes, we've been working on an edit feature since last year!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6451edee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "social\n",
      "media\n",
      "firm\n",
      "'s\n",
      "communications\n",
      "team\n",
      "tweeted\n",
      ":\n",
      "'\n",
      "Now\n",
      "that\n",
      "everyone\n",
      "is\n",
      "asking\n",
      "…\n",
      "yes\n",
      ",\n",
      "we\n",
      "'ve\n",
      "been\n",
      "working\n",
      "on\n",
      "an\n",
      "edit\n",
      "feature\n",
      "since\n",
      "last\n",
      "year\n",
      "!\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51cda6",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "There are a few types of preprocessing to improve how we model with words. The first is `lemmatizing.` The `lemma` of a word is its base form. For example, **\"walk\"** is the lemma of the word **\"walking\"**. \n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fcd87a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\t\t\tLemma \t\t\t\tStopword\n",
      "--------------------------------------------------------------------------------\n",
      "The\t\t\t\tthe\t\t\t\tTrue\n",
      "social\t\t\t\tsocial\t\t\t\tFalse\n",
      "media\t\t\t\tmedium\t\t\t\tFalse\n",
      "firm\t\t\t\tfirm\t\t\t\tFalse\n",
      "'s\t\t\t\t's\t\t\t\tTrue\n",
      "communications\t\t\t\tcommunication\t\t\t\tFalse\n",
      "team\t\t\t\tteam\t\t\t\tFalse\n",
      "tweeted\t\t\t\ttweet\t\t\t\tFalse\n",
      ":\t\t\t\t:\t\t\t\tFalse\n",
      "'\t\t\t\t'\t\t\t\tFalse\n",
      "Now\t\t\t\tnow\t\t\t\tTrue\n",
      "that\t\t\t\tthat\t\t\t\tTrue\n",
      "everyone\t\t\t\teveryone\t\t\t\tTrue\n",
      "is\t\t\t\tbe\t\t\t\tTrue\n",
      "asking\t\t\t\task\t\t\t\tFalse\n",
      "…\t\t\t\t…\t\t\t\tFalse\n",
      "yes\t\t\t\tyes\t\t\t\tFalse\n",
      ",\t\t\t\t,\t\t\t\tFalse\n",
      "we\t\t\t\twe\t\t\t\tTrue\n",
      "'ve\t\t\t\t've\t\t\t\tTrue\n",
      "been\t\t\t\tbe\t\t\t\tTrue\n",
      "working\t\t\t\twork\t\t\t\tFalse\n",
      "on\t\t\t\ton\t\t\t\tTrue\n",
      "an\t\t\t\tan\t\t\t\tTrue\n",
      "edit\t\t\t\tedit\t\t\t\tFalse\n",
      "feature\t\t\t\tfeature\t\t\t\tFalse\n",
      "since\t\t\t\tsince\t\t\t\tTrue\n",
      "last\t\t\t\tlast\t\t\t\tTrue\n",
      "year\t\t\t\tyear\t\t\t\tFalse\n",
      "!\t\t\t\t!\t\t\t\tFalse\n",
      "'\t\t\t\t'\t\t\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\t\\t\\tLemma \\t\\t\\t\\tStopword\".format('Token','Lemma','Stopword'))\n",
    "print(\"-\"*80)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text}\\t\\t\\t\\t{token.lemma_}\\t\\t\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe13d7",
   "metadata": {},
   "source": [
    "Language data has a lot of noise mixed in with informative content. Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarly helps by combining multiple forms of the same word into one base form. However, lemmatizing and dropping stopwords might result in the models performing worse. This preprocessing should be treated as part of the `hyperparameter optimization process`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f594262",
   "metadata": {},
   "source": [
    "## Pattern Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908589b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "076a6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90db93",
   "metadata": {},
   "source": [
    "The matcher is created using the vocabulary of the model. Setting **attr='LOWER'** will match the phrases on lowercased text. This provides `case insensitive` matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f08f1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The rockets will be made by Arianespace, which was founded by Amazon owner Jeff Bezos and United Launch Alliance. Like Elon Musk's Starlink, users will connect to the internet via a terminal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "196a5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"Starlink\",\"Arianaspace\",\"Amazon\",\"United Launch Alliance\",\"Elon Musk\",\"Jeff Bezos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffdf27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starlink, Arianaspace, Amazon, United Launch Alliance, Elon Musk, Jeff Bezos]\n"
     ]
    }
   ],
   "source": [
    "patterns = [nlp(text) for text in terms]\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a44d14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"TerminologyList\",patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26e78fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 1, 3), (3766102292120407359, 19, 20)]\n"
     ]
    }
   ],
   "source": [
    "text_doc = nlp(\"Unlike Elon Musk's Falcon rockets, the rockets used for Project Kuiper's are still in development.\" \n",
    "               \"Amazon says Project Kuiper aims to provide high-speed broadband to customers.\")\n",
    "\n",
    "matches = matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fb64fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList Elon Musk\n",
      "TerminologyList Amazon\n"
     ]
    }
   ],
   "source": [
    "match_id_1, start_1, end_1 = matches[0]\n",
    "match_id_2, start_2, end_2 = matches[1]\n",
    "\n",
    "\n",
    "print(nlp.vocab.strings[match_id_1], text_doc[start_1:end_1])\n",
    "print(nlp.vocab.strings[match_id_2], text_doc[start_2:end_2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
